{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5b6q/hK0Bzn+LrvqfZzQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NayanaSaikumar21/twitter_sentiment_analysis/blob/main/twitterSentimentAnalysisUsingLstm_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBcCz19vpukf"
      },
      "outputs": [],
      "source": [
        "# ---- STEP 0: Install Required Packages ----\n",
        "!pip install numpy pandas tensorflow nltk tqdm gradio kaggle\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import gradio as gr\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import files\n",
        "\n",
        "# Ensure NLTK resources are available\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# ---- STEP 1: Upload & Configure Kaggle API ----\n",
        "print(\"üìÇ Please upload your 'kaggle.json' file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move kaggle.json to the correct directory\n",
        "kaggle_json_path = \"kaggle.json\"\n",
        "kaggle_dest_path = \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "shutil.move(kaggle_json_path, kaggle_dest_path)\n",
        "os.chmod(kaggle_dest_path, 0o600)\n",
        "\n",
        "print(\"‚úÖ Kaggle API key configured successfully.\")\n",
        "\n",
        "# ---- STEP 2: Download & Extract Dataset ----\n",
        "!kaggle datasets download -d kazanova/sentiment140 -p /mnt/data --unzip\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = \"/mnt/data/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "# Check if dataset exists\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"‚ùå {DATA_PATH} not found. Please check Kaggle download.\")\n",
        "\n",
        "print(\"‚úÖ Dataset downloaded successfully.\")\n",
        "\n",
        "# ---- STEP 3: Load & Preprocess Dataset ----\n",
        "# Load dataset\n",
        "columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "df = pd.read_csv(DATA_PATH, names=columns, encoding=\"latin-1\")[[\"target\", \"text\"]]\n",
        "\n",
        "# Convert sentiment labels\n",
        "df[\"target\"] = df[\"target\"].replace({0: 0, 4: 1})\n",
        "\n",
        "# Check original class distribution\n",
        "print(\"üìä Original Class Distribution:\")\n",
        "print(df[\"target\"].value_counts())\n",
        "\n",
        "# Balance dataset: Take 50K positive & 50K negative samples\n",
        "negative_samples = df[df[\"target\"] == 0].sample(n=50000, random_state=42)\n",
        "positive_samples = df[df[\"target\"] == 1].sample(n=50000, random_state=42)\n",
        "df_balanced = pd.concat([negative_samples, positive_samples]).sample(frac=1, random_state=42)\n",
        "\n",
        "# Check new class distribution\n",
        "print(\"üìä Balanced Class Distribution:\")\n",
        "print(df_balanced[\"target\"].value_counts())\n",
        "\n",
        "\n",
        "# ---- STEP 4: Text Cleaning ----\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):  # Handle NaN values\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
        "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)  # Remove hashtags\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(\n",
        "    lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words])\n",
        ")\n",
        "\n",
        "# Remove empty rows\n",
        "df = df[df[\"clean_text\"] != \"\"]\n",
        "\n",
        "# Ensure dataset is not too small\n",
        "if len(df) < 100:\n",
        "    raise ValueError(f\"‚ùå Not enough data to train! Dataset contains only {len(df)} samples. Add more data.\")\n",
        "\n",
        "print(f\"‚úÖ Data preprocessing complete. {len(df)} samples available.\")\n",
        "\n",
        "# ---- STEP 5: Tokenization ----\n",
        "MAX_NUM_WORDS = 10000  # Vocabulary size\n",
        "MAX_SEQ_LENGTH = 50     # Sequence length\n",
        "EMBEDDING_DIM = 200     # Embedding size\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
        "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
        "\n",
        "print(\"‚úÖ Tokenization complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---- STEP 6: Define & Train LSTM Model ----\n",
        "model = Sequential([\n",
        "    Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQ_LENGTH),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(\"‚úÖ LSTM Model Defined.\")\n",
        "\n",
        "# ---- STEP 7: Train the Model ----\n",
        "if len(df) > 100:\n",
        "    print(\"üöÄ Training model with validation split...\")\n",
        "    history = model.fit(padded_sequences, df[\"target\"].values, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "print(\"‚úÖ Model training complete.\")\n",
        "\n",
        "# ---- STEP 8: Save the Model ----\n",
        "MODEL_PATH = \"/mnt/data/sentiment_lstm_model.h5\"\n",
        "model.save(MODEL_PATH)\n",
        "print(f\"‚úÖ Model saved at {MODEL_PATH}\")\n"
      ],
      "metadata": {
        "id": "vM7c_lCspyiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm  # Progress tracking\n",
        "import numpy as np\n",
        "\n",
        "# ---- STEP 9: Load & Compile Model ----\n",
        "print(\"üîÑ Loading model...\")\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  # Fixes warning\n",
        "\n",
        "# ---- STEP 10: Optimize Sentiment Prediction ----\n",
        "print(\"üîÑ Preprocessing dataset for prediction...\")\n",
        "\n",
        "# Clean and tokenize all texts at once (Vectorized for speed)\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: \" \".join(\n",
        "    [lemmatizer.lemmatize(word) for word in clean_text(x).split() if word not in stop_words]\n",
        "))\n",
        "\n",
        "# Convert all texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
        "\n",
        "print(\"üöÄ Running batch predictions...\")\n",
        "\n",
        "# Predict in a single batch instead of one by one (MUCH FASTER)\n",
        "batch_predictions = model.predict(padded_sequences, verbose=1)  # Predict all at once\n",
        "\n",
        "# Convert predictions to labels\n",
        "df[\"predicted_sentiment\"] = np.where(batch_predictions.flatten() > 0.5, \"Positive üòä\", \"Negative üò†\")\n",
        "\n",
        "# ---- STEP 11: Save Predictions to CSV ----\n",
        "output_file = \"/mnt/data/predicted_sentiments.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"‚úÖ Sentiment predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "SX8vNmFtp3vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---- STEP 12: Create a Gradio Interface ----\n",
        "def gradio_predict(input_text):\n",
        "    return predict_sentiment(input_text)\n",
        "\n",
        "interface = gr.Interface(fn=gradio_predict, inputs=\"text\", outputs=\"text\")\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "ypRLECV4p6bO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}